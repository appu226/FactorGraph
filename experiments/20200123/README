*********************************************
*** Running the factor graph algorithm    ***
*** on the bdd blif bench mark test cases ***
*********************************************

1. How to run
   Go to the root source folder and run:
     experiments/20190201/run_all_test_cases.sh

2. Test cases
   The test has been run on the bdd_blif data set, which consist of
     a set of digital circuits in th blif format.
   The location of the input data can be customized as a parameter 
     to the blif_solve command in run_one_test_case.sh.
   Each circuit has, at the base level, 4 types of variables:
   - primary input variables v_pi
   - primary output variables v_po
   - latch input variables v_li
   - latch output variables v_lo

    v_pi and v_lo are input variables to the circuits, using which
      the blif files defines v_po and v_li, which are
      the output variables.

   We define our factors by assigining the latch input variables
     to their circuits.
       F_k = v_li_k <-> circuit(v_li_k)
   The aim is to compute 
     \exists v_pi (F_1 \and F_2 \and ... \and F_n)

   *************************************************
   *** In other words, we are projecting         ***
   *** the conjunction of all the factors        ***
   *** onto all the non primary input variables. ***
   *************************************************



3. Tests for each case
   For each data set, (starting from the smallest by file size to the largest),
     we loop over many largest_support_set sizes, and num_convergences.
   The meanings of these parameters are as follows:
   --- largest_support_set: The maximum number of variables in a merged variable node or func node
           We use the following algorithms for merging the set of nodes in the graph:
           1. compute the disconnected partitions among the factors
           2. for each partition:
           3.     let L = list of nodes to merge (pairs of funcs and pairs of vars)
           4.     for each pair in L, compute the compatibility
                           (compatibility(x, y) = |SS(x) \intersect SS(Y)| / min(|SS(x)|, |SS(y)|)
                            where SS stands for support set)
           5.     turn L into a max heap based on the compatibilities
           6.     while L is not empty:
           7.         extract the most compatible pair (x, y) from L
           8.         if |SS(x) \union SS(y)| <= Largest_Support_Set:
           9.             remove pairs with x and/or y from L
           10.            compute node z = merge(x, y)
           11.            for each neighbour n of z:
           12.                add (z, n) into L while maintaining the max-heap property
   --- number_of_convergences: Number of times to run the factor graph algorithm
           We run the factor graph algorithm to convergence,
           and compute the result as the set of final messages coming into the non-PI variable nodes.
           We then take these messages as an additional set of factors,
           and re-run the algorithm from scratch (including the random merging of) 
                                                 (variable and function nodes    ).
           and thus compute a new result, guaranteed to be no-less-tighter than the first result.
           This we repeat "number_of_convergences" times.
   For each data set, for each combination of the above mentioned parameters,
     the computations were bounded by 2 mins.
   The computations were run with -O3 flag on a Intel(R) Core(TM) i3-4030U CPU @ 1.90GHz chip
   with 3.8G RAM plus 3.7G Swap memory.

4. Results
   The results are summarized in a table in
     summary.txt
   Detailed logs from which results have been scraped are compressed in:
     experiments/20190201/results.tgz


